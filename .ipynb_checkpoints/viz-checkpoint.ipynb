{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "from utils import PRIORS\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alpha_mask(masks, inputs, fig_size, num_samples=5, slot_plot=False, save=False):\n",
    "    \"\"\"Plot and save masks of random samples from eval batch\"\"\"\n",
    "    idxs = np.random.randint(32, size=num_samples)\n",
    "    K, L = masks.shape[1], masks.shape[2]\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    colors = [(0.9, 0.0, 0.0), (0.0, 1.0, 0.0), (0.0, 0.0, 1.0), (1.0, 0.5, 0.0), \n",
    "              (0.5, 0.0, 1.0), (1.0, 0.0, 1.0), (1.0, 1.0, 0.0)]\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(num_samples, 1, i+1)\n",
    "        if not slot_plot:\n",
    "            plt.xticks(np.arange(L), inputs[idxs[i], :].astype(int))\n",
    "        for s_k in range(K):\n",
    "            # plot alpha_masks\n",
    "            plt.plot(range(L), masks[idxs[i], s_k, :, 0], color=colors[s_k], linestyle='solid')\n",
    "                \n",
    "    plt.tight_layout(pad=0.5, w_pad=0.5)\n",
    "    plt.show()\n",
    "    if save:\n",
    "        plt.savefig(\"mask-plt-\" + map(str, idxs) + \".png\", dpi=300)\n",
    "                \n",
    "        \n",
    "def plot_attention_head(inputs, outputs, attention):\n",
    "    ax = plt.gca()\n",
    "    ax.matshow(attention[0])\n",
    "\n",
    "    ax.set_xticklabels(inputs[0])\n",
    "    ax.set_yticklabels(outputs[0])\n",
    "\n",
    "    ax.set_xticks(range(len(inputs[0])))\n",
    "    ax.set_yticks(range(len(outputs[0])))\n",
    "    \n",
    "\n",
    "def plot_self_attn_weights(inputs, outputs, attention_heads, save=False):\n",
    "    fig = plt.figure(figsize=(25, 25))\n",
    "    idx = np.random.randint(inputs.shape[0], size=1)\n",
    "    num_heads = attention_heads.shape[1]\n",
    "    for h in range(num_heads):\n",
    "        ax = fig.add_subplot((num_heads // 4)+1, 4, h+1)\n",
    "        plot_attention_head(inputs[idx], outputs[idx], attention_heads[idx, h])\n",
    "        ax.set_xlabel(f'Head {h+1}')\n",
    "\n",
    "    plt.tight_layout(pad=0.5, w_pad=0.2, h_pad=0.2)\n",
    "    plt.show()\n",
    "    if save:\n",
    "        plt.savefig(\"attn-weights-plt-\" + str(idx) + \".png\", dpi=300)\n",
    "\n",
    "\n",
    "def plot_bw_fw_access(masks, slot_attn_weights, inputs, fig_size, num_samples=1, \n",
    "                      pad_token=5, dataset_id=\"craft\", dataset_fname=\"makeallf\", \n",
    "                      save=False):\n",
    "    \"\"\"Plot and save qualitative viz of BW-FW access -- alpha-masks v/s slot-attn coeffs.\"\"\"\n",
    "    colors = [(0.9, 0.0, 0.0), (0.0, 0.5, 0.0), (0.0, 0.0, 1.0), (1.0, 0.65, 0.0), \n",
    "              (0.5, 0.0, 1.0), (1.0, 0.0, 1.0), (1.0, 1.0, 0.0)]\n",
    "    CRAFT_VOCAB = [' \\u2193 ', ' \\u2191 ', ' \\u2190 ', ' \\u2192 ', ' u ', ' D ']\n",
    "    MGRID_VOCAB = [' \\u2190 ', ' \\u2192 ', ' \\u2191 ', ' PICK ', ' DROP ', ' TOGL ', ' DONE ']\n",
    "    bsz = inputs.shape[0]\n",
    "    idx = np.random.randint(bsz, size=num_samples)[0]\n",
    "    K = masks.shape[1]\n",
    "    # y-axis min/max ranges for mask & SA coeffs\n",
    "    mask_min, mask_max = 0.0, 0.4\n",
    "    sa_min, sa_max = 0.6, 1.0\n",
    "    # convert inputs to int\n",
    "    inputs = inputs.astype(int)\n",
    "    if dataset_id == \"craft\":\n",
    "        action_tokens = np.asarray([CRAFT_VOCAB[a] for a in inputs[idx, :]])\n",
    "    elif dataset_id == \"minigrid\":\n",
    "        action_tokens = np.asarray([MGRID_VOCAB[a] for a in inputs[idx, :]])\n",
    "    non_padded_L = np.amin(np.where(inputs[idx, :] == pad_token)[0])\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    # plot K separate panels of slot_attn v/s alpha_masks\n",
    "    for s in range(K):\n",
    "        plt.subplot(K, 1, s+1)\n",
    "        plt.xticks(np.arange(non_padded_L), action_tokens)\n",
    "        plt.yticks([mask_min, mask_max, sa_min, sa_max], ['off', 'on', 'off', 'on'])\n",
    "        plt.ylim([0.0, 1.1])\n",
    "        # additional processing for bw-fw-plot\n",
    "        alpha_max, alpha_min = non_padded_L, 0\n",
    "        # binarize values\n",
    "        mask = (masks[idx, s, :, 0] > 0.8).astype(float)\n",
    "        # indices where alpha-mask is \"on\"\n",
    "        mask_on_idxs = np.where(mask)[0]\n",
    "        slot_attn = (slot_attn_weights[idx, :, s] > 0.8).astype(float)\n",
    "        if not mask_on_idxs.size == 0:\n",
    "            # \"filter\" out only past and future timesteps of slot_attn_coeffs\n",
    "            alpha_min = np.amin(mask_on_idxs)\n",
    "            alpha_max = np.clip(np.amax(mask_on_idxs), 0, non_padded_L)\n",
    "        # plot slot_attn_coeffs\n",
    "        filtered_sa_coeffs = np.zeros((non_padded_L,))\n",
    "        # \"past\" indices \n",
    "        filtered_sa_coeffs[0:alpha_min] = slot_attn[0:alpha_min]\n",
    "        # \"future\" indices \n",
    "        filtered_sa_coeffs[alpha_max:non_padded_L] = slot_attn[alpha_max:non_padded_L]\n",
    "        # re-norm mask [0, 0.45] & SA [0.55, 1.0] coeffs in their respective ranges\n",
    "        norm_mask = mask * mask_max\n",
    "        norm_filtered_sa_coeffs = (filtered_sa_coeffs * (sa_max - sa_min)) + sa_min\n",
    "        # plot lines -- alpha_mask & slot_attn\n",
    "        plt.plot(range(non_padded_L), norm_mask[0:non_padded_L], color=colors[s], \n",
    "                 linestyle='solid', linewidth=2.0)\n",
    "        plt.axhline(y=0.5, color='k', linestyle='-', linewidth=1.0)\n",
    "        plt.plot(range(non_padded_L), norm_filtered_sa_coeffs, color=colors[s], \n",
    "                 linestyle='dotted', linewidth=2.0)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(\"bw-fw-access-\" + dataset_fname + \"-\" + str(idx) + \".png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Viz decoder alpha masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data logs\n",
    "dataset = \"craft-skills/\"\n",
    "f_dir = \"hcrmc/logs/\"\n",
    "eval_step = 9\n",
    "f_name = \"eval_logs_\" + str(eval_step) + \".npz\"\n",
    "data = np.load(dataset + f_dir + f_name, allow_pickle=True)\n",
    "# plot decoder alpha masks\n",
    "plot_alpha_mask(data['masks'], data['actions'], (10, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Viz self-attention of Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data logs\n",
    "dataset = \"\" \n",
    "f_dir = \"\"\n",
    "eval_step = -1\n",
    "f_name = \"eval_logs_\" + str(eval_step) + \".npz\"\n",
    "data = np.load(dataset + f_dir + f_name, allow_pickle=True)\n",
    "plot_self_attn_weights(data['actions'].astype(int), data['actions'].astype(int), data['enc_attn_weights'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Viz slot-attention weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data logs\n",
    "dataset = \"\"\n",
    "f_dir = \"\"\n",
    "eval_step = -1\n",
    "f_name = \"eval_logs_\" + str(eval_step) + \".npz\"\n",
    "data = np.load(dataset + f_dir + f_name, allow_pickle=True)\n",
    "idxs = [3, 5, 7, 11, 13]\n",
    "# plot slot-attn coeffs\n",
    "plot_alpha_mask(np.transpose(data['slot_attn_weights'], axes=[0, 2, 1]), data['actions'], idxs, \n",
    "          (10, 8), slot_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: viz BW-FW Access -- slot_attn v/s alpha_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from logs\n",
    "root_dir = \"\"\n",
    "logs_dir = \"\"\n",
    "eval_step = -1\n",
    "f_name = \"eval_logs_\" + str(eval_step) + \".npz\"\n",
    "data = np.load(root_dir + logs_dir + f_name, allow_pickle=True)\n",
    "# plot decoder alpha masks\n",
    "plot_bw_fw_access(data['masks'], data['slot_attn_weights'], data['actions'], (9, 6), \n",
    "                  pad_token=6, dataset_id=\"minigrid\", dataset_fname=\"ulpkp\", save=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram plots of empirical prior dist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = PRIORS[\"keycorridor-s4r3\"]\n",
    "plt.xlim([1, len(prior)])\n",
    "plt.xticks(np.arange(1, len(prior)+1, 1))\n",
    "plt.xlabel('Number of sub-routines')\n",
    "plt.ylabel('Halting probability')\n",
    "plt.title('Empirical prior distribution')\n",
    "plt.bar(list(range(1, len(prior)+1, 1)), prior, width=0.2)\n",
    "plt.grid(True)\n",
    "plt.savefig(\"emp-prior-kcs4r3.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
